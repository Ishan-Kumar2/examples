{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert-pytorch-to-ignite.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo0JaCAvVI64"
      },
      "source": [
        "<!-- ---\n",
        "title: How to convert pure PyTorch code to Ignite\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - training loop\n",
        "  - validation loop\n",
        "--- -->\n",
        "# How to convert pure PyTorch code to Ignite "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXNZ4XPeV8_I"
      },
      "source": [
        "In this guide we will demonstrate how PyTorch code compenents can be converted into compact and flexible PyTorch-Ignite code. Since Ignite focusses on the training and validation pipeline, the code for models, datasets, optimizers, etc will remain user-defined and in pure PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EmmpiTX6huF"
      },
      "source": [
        "## Training Loop to `trainer`\n",
        "\n",
        "First, we need to move the code or steps taken to process a single batch of data while training under a function (`train_step()` below). This function will take `engine` and `batch` (current batch of data) as arguments and can return any data (usually the loss) that can be accessed via `engine.state.output`. We pass this function to `Engine` which creates a `trainer` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDkeEWz58hCJ"
      },
      "source": [
        "```python\n",
        "def train_step(engine, batch):\n",
        "    inputs, targets = batch\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MWJzKK8-AiC"
      },
      "source": [
        "There are other [helper methods](https://pytorch.org/ignite/engine.html#helper-methods-to-define-supervised-trainer-and-evaluator) that directly create the `trainer` object without writing a custom function for some common use cases like [supervised training](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html#ignite.engine.create_supervised_trainer) and [truncated backprop through time](https://pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.tbptt.create_supervised_tbptt_trainer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cocfuUFZ8okw"
      },
      "source": [
        "## Validation Loop to `evaluator`\n",
        "\n",
        "Next we will move the model evaluation logic under another function (`validation_step()` below) which receives the same parameters as `train_step()` and processes a single batch of data to return some output (usually the predicted and actual value which can be used to calculate metrics) stored in `engine.state.output`. Another instance (called `evaluator` below) of `Engine` is created by passing the `validation_step()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gFQq-zD8onk"
      },
      "source": [
        "```python\n",
        "def validation_step(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x, y = prepare_batch(batch)\n",
        "        y_pred = model(x)\n",
        "\n",
        "    return y_pred, y\n",
        "    \n",
        "evaluator = Engine(validation_step)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAIBqfFm8oqS"
      },
      "source": [
        "Similar to the training loop, there are [helper methods](https://pytorch.org/ignite/engine.html#helper-methods-to-define-supervised-trainer-and-evaluator) to avoid writing this custom evaluation function like [`create_supervised_evaluator`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_evaluator.html#ignite.engine.create_supervised_evaluator).\n",
        "\n",
        "Note that you can create different evaluators for training, validation and testing if they serve different functions. A common practice is to have two separate evaluators for training and validation, since the results of the validation evaluator is helpful in determining the best model to save."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t4PsYXn8ost"
      },
      "source": [
        "## Switch to built-in Metrics\n",
        "\n",
        "Then we can replace code for calculating metrics like accuracy and instead use several [out-of-the-box metrics](https://pytorch.org/ignite/metrics.html#complete-list-of-metrics) that Ignite provides or write a custom one (refer [here](https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric)). The metrics will be computed using the `evaluator`'s output. Finally, we attach these metrics to the `evaluator`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBKcAo188ovi"
      },
      "source": [
        "```python\n",
        "metrics = {\n",
        "  \"accuracy\": Accuracy()\n",
        "  ...\n",
        "}\n",
        "\n",
        "for name, metric in metrics.items():\n",
        "    metric.attach(evaluator, name)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnGK925N5AR7"
      },
      "source": [
        "## Organising code into Events and Handlers\n",
        "\n",
        "Next we need to identify any code that is triggered after an event is completed. Example of events can be start of an iteration, completion or an epoch or even start of backprop. We already provide some predefined events (complete list [here](https://pytorch.org/ignite/generated/ignite.engine.events.Events.html#ignite.engine.events.Events)) however we can also create custom ones (refer [here](https://pytorch.org/ignite/concepts.html#custom-events)). We move the event specific code to different handlers (named functions, lambdas, class functions) which are attached to these events and executed whenever an event is triggered. Here are some common handlers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZIdI39b-rB4"
      },
      "source": [
        "### Running `evaluator` and logging metrics\n",
        "\n",
        "When the `trainer` completes an epoch, we have to run the `evaluator` on the training/validation/test dataset to get predictions. This can be done automatically via a handler function aadded to a built-in event `EPOCH_COMPLETED` like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy8ARbvv_t-n"
      },
      "source": [
        "```python\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def run_train_validation():\n",
        "    evaluator.run(train_loader)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRgDrTgi5AU_"
      },
      "source": [
        "### Progress Bar\n",
        "\n",
        "Instead of a simple `tqdm` or writing a custom progress bar in the `train_step()`, we can attach a built-in `ProgressBar()` to the trainer that updates the loss returned by the `train_step()` in real time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llBggNW95AXW"
      },
      "source": [
        "```python\n",
        "ProgressBar().attach(trainer, output_transform=lambda x: {'batch loss': x})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkqMcVnA5AZ3"
      },
      "source": [
        "### Checkpointing\n",
        "\n",
        "Instead of comparing the metrics after the evaluator is run to figure out the best model, we can use the built-in `Checkpoint()` method to do it for us:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmV4-35W5Ady"
      },
      "source": [
        "```python\n",
        "def score_function(engine):\n",
        "    return engine.state.metrics[\"accuracy\"]\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    \"checkpoints\",\n",
        "    n_saved=2,\n",
        "    filename_prefix=\"best\",\n",
        "    score_function=score_function,\n",
        "    score_name=\"accuracy\",\n",
        "    global_step_transform=global_step_from_engine(trainer),\n",
        ")\n",
        "\n",
        "evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\n",
        "```"
      ]
    }
  ]
}