# -*- coding: utf-8 -*-
"""Machine Translation using PyTorch Ignite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YIFdKOjc7tGhiqWzmCoJPJW5Gd_FidG2
"""

# # installing appropriate modules

# setup TPU environment
# import os
# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'

# os.environ

# VERSION = "nightly"
# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
# !python pytorch-xla-env-setup.py --version $VERSION > /dev/null

import torch

from pathlib import Path
from datasets import load_dataset
from transformers import (MBartForConditionalGeneration, MBartTokenizer)
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import random_split

import ignite
import ignite.distributed as idist
from ignite.contrib.engines import common
from ignite.metrics import Loss
from ignite.metrics.nlp import RougeN
from ignite.metrics.nlp import Bleu
from ignite.utils import manual_seed, setup_logger
from ignite.engine import Engine, Events


import ignite
import ignite.distributed as idist
from ignite.contrib.engines import common
from ignite.contrib.handlers import PiecewiseLinear
from ignite.engine import Engine, Events
from ignite.handlers import Checkpoint, global_step_from_engine
from ignite.metrics import Accuracy, Loss
from ignite.utils import manual_seed, setup_logger

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast

with_amp = True
seed = 126
num_epochs = 10
output_path_ = '/content'
checkpoint_every = 100
model_name = "sshleifer/tiny-mbart"
tokenizer_name = "sshleifer/tiny-mbart"

"""## Preparing data

We will be using IITB parallel corpus (hi - en) for this example .
"""

from datasets import load_dataset

dataset = load_dataset("stas/wmt16-en-ro-pre-processed")

dataset

print("Example of a Datapoint")
print(dataset['train'][0])
print("Lengths")
print("\t Train Set - {}".format(len(dataset['train'])))
print("\t Val Set - {}".format(len(dataset['validation'])))
print("\t Test Set - {}".format(len(dataset['test'])))

class TransformerDataset(torch.utils.data.Dataset):
    def __init__(self, data, src_text_id, tgt_text_id, tokenizer, max_length):
        self.data = data
        self.src_text_id = src_text_id
        self.tgt_text_id = tgt_text_id
        self.tokenizer = MBartTokenizer.from_pretrained(tokenizer_name, src_lang="en_XX", tgt_lang="ro_RO")
        self.max_length = max_length

    def __getitem__(self, idx):
        src_text = [str(self.data[idx]['translation'][self.src_text_id])]
        tgt_text = [str(self.data[idx]['translation'][self.tgt_text_id])]
        
        src_text = self.tokenizer(src_text,max_length= self.max_length, padding = 'max_length',truncation=True)
        with self.tokenizer.as_target_tokenizer():
          tgt_text = self.tokenizer(tgt_text,max_length= self.max_length,padding = 'max_length',truncation=True)

        # src_text = self.tokenizer(src_text, return_tensors="pt")
        # print(batch)
        # with self.tokenizer.as_target_tokenizer():
        # tgt_text = self.tokenizer(tgt_text, return_tensors="pt").input_ids
        # print(src_text['input_ids'])
        return {
            "input_ids": torch.tensor(src_text['input_ids']).squeeze(0),
            "attention_mask": torch.tensor(src_text['attention_mask']).squeeze(0),
            "tgt": torch.tensor(tgt_text['input_ids']).squeeze(0),
        }

    def __len__(self):
        return len(self.data)

train_dataset = TransformerDataset(dataset['train'], 'en', 'ro', True, 32)
val_dataset = TransformerDataset(dataset['validation'], 'en', 'ro', True, 32)

train_dataset[88]['tgt'].shape

for i in range(1000):
  assert train_dataset[i]['attention_mask'].shape[0]==32,i
  assert train_dataset[i]['tgt'].shape[0]==32,print(i,train_dataset[i]['tgt'].shape)
  
  assert train_dataset[i]['input_ids'].shape[0]==32,i

"""## Initiating model and trainer for training"""

# Create Trainer 
def create_trainer(model, optimizer, criterion, with_amp, train_sampler, logger):

    device = idist.device()
    scaler = GradScaler(enabled=with_amp)

    def train_step(engine, batch):
        src_ids = batch["input_ids"]
        src_attention_mask = batch["attention_mask"]
        tgt = batch["input_ids"]

        if src_ids.device != device:
            src_ids = src_ids.to(device, non_blocking=True, dtype=torch.long)
            src_attention_mask = src_attention_mask.to(device, non_blocking=True, dtype=torch.long)
            tgt = tgt.to(device, non_blocking=True, dtype=torch.long)

        model.train()

        with autocast(enabled=with_amp):
            y_pred = model(src_ids, src_attention_mask)['logits']
            y_pred = y_pred.view(-1, y_pred.size(2))
            tgt = tgt.contiguous().view(-1)
            # print(y_pred.shape, tgt.shape, str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')
            loss = criterion(y_pred, tgt)

        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        return {
            "batch loss": loss.item(),
        }
    
    trainer = Engine(train_step)
    trainer.logger = logger

    to_save = {"trainer": trainer, "model": model, "optimizer": optimizer}
    metric_names = [
        "batch loss",
    ]

    common.setup_common_training_handlers(
        trainer=trainer,
        train_sampler=train_sampler,
        output_names=metric_names,
        clear_cuda_cache=False,
    )
    return trainer

# Let's now setup evaluator engine to perform model's validation and compute metrics
def create_evaluator(model, metrics, with_amp, tag="val"):

    device = idist.device()

    @torch.no_grad()
    def evaluate_step(engine, batch):
        model.eval()

        src_ids = batch["input_ids"]
        src_attention_mask = batch["attention_mask"]
        tgt = batch["input_ids"]

        if src_ids.device != device:
            src_ids = src_ids.to(device, non_blocking=True, dtype=torch.long)
            src_attention_mask = src_attention_mask.to(device, non_blocking=True, dtype=torch.long)
            tgt = tgt.to(device, non_blocking=True, dtype=torch.long)
            
        with autocast(enabled=with_amp):
            y_pred = model(src_ids, src_attention_mask)
        return y_pred

    evaluator = Engine(evaluate_step)

    for name, metric in metrics.items():
        metric.attach(evaluator, name)
    
    if idist.get_rank() == 0:
        common.ProgressBar(desc=f"Evaluation ({tag})", persist=False).attach(evaluator)

    return evaluator

def initialize():
  # model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50")
  model = MBartForConditionalGeneration.from_pretrained(model_name)
  lr = 5e-5 * idist.get_world_size()
  model = idist.auto_model(model)
  optimizer = optim.AdamW(model.parameters(), lr=lr)
  optimizer = idist.auto_optim(optimizer)
  criterion = nn.CrossEntropyLoss(ignore_index = train_dataset.tokenizer.pad_token_id, reduction='sum')
  # le = config["num_iters_per_epoch"]
  # milestones_values = [
  #       (0, 0.0),
  #       (le * warmup_epochs, lr),
  #       (le * num_epochs, 0.0),
  # ]
  # lr_scheduler = PiecewiseLinear(optimizer, param_name="lr", milestones_values=milestones_values)
  return model, optimizer, criterion

def get_dataloaders(train_dataset, val_dataset, batch_size=64, num_workers=2):

    # Setup data loader also adapted to distributed config: nccl, gloo, xla-tpu
    train_loader = idist.auto_dataloader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True,
    )

    val_loader = idist.auto_dataloader(
        val_dataset, batch_size=2 * batch_size, num_workers=num_workers, shuffle=False,
    )
    return train_loader, val_loader

idist.device()



def training(local_rank):

    rank = idist.get_rank()
    manual_seed(seed + rank)
    device = idist.device()

    logger = setup_logger(name="NMT", distributed_rank=local_rank)
    
    train_loader, val_loader = get_dataloaders(train_dataset,val_dataset)
    model, optimizer, criterion = initialize()
    
    trainer = create_trainer(model, optimizer, criterion, with_amp, train_loader.sampler, logger)

    metrics = {
        "Bleu": Bleu(ngram=4, smooth="smooth1"),
        "Loss": Loss(criterion),
    }

    evaluator = create_evaluator(model, metrics, with_amp, tag="val")
    train_evaluator = create_evaluator(model, metrics, with_amp, tag="train")
    
    @trainer.on(Events.EPOCH_COMPLETED(every=2))
    def run_validation(engine):
        epoch = trainer.state.epoch
        state = train_evaluator.run(train_loader)
        log_metrics(logger, epoch, state.times["COMPLETED"], "Train", state.metrics)
        state = evaluator.run(val_loader)
        log_metrics(logger, epoch, state.times["COMPLETED"], "Validation", state.metrics)

    if rank == 0:
      now = datetime.now().strftime("%Y%m%d-%H%M%S")
      folder_name = f"Translation_Model_backend-{idist.backend()}-{idist.get_world_size()}_{now}"
      output_path = Path(output_path_) / folder_name
      if not output_path.exists():
            output_path.mkdir(parents=True)
      
      logger.info(f"Output path: {output_path}")

    if rank == 0:
        # Setup TensorBoard logging on trainer and evaluators. Training Losses and Eval Metrics are logged
        # evaluators = {"training": train_evaluator, "test": evaluator}
        
        # tb_logger = common.setup_tb_logging(
        #     output_path, trainer, optimizer, evaluators=evaluators, log_every_iters=100
        # )
    
    try:
        trainer.run(train_loader, max_epochs=num_epochs)
    except Exception as e:
        logger.exception("")
        raise e
    
    if rank == 0:
        tb_logger.close()

def run():
    with idist.Parallel(backend="nccl", nproc_per_node=2) as parallel:
        parallel.run(training)

run()
