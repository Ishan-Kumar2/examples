{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Machine Translation using PyTorch Ignite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishan-Kumar2/examples/blob/mt_example/Examples/Machine_Translation_using_PyTorch_Ignite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUsi2Pv957NV"
      },
      "source": [
        "# installing appropriate modules\n",
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git@master;\n",
        "!pip install git+https://github.com/huggingface/datasets.git@master;\n",
        "!pip install git+https://github.com/pytorch/ignite.git@master;\n",
        "!pip install sentencepiece;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZNpWSE_8GOZ"
      },
      "source": [
        "## Uncomment if using TPU\n",
        "# setup TPU environment\n",
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "# VERSION = \"nightly\"\n",
        "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNj9cx216vqT"
      },
      "source": [
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "import nltk\n",
        "import ignite\n",
        "import ignite.distributed as idist\n",
        "from ignite.contrib.engines import common\n",
        "from ignite.metrics import Loss, RougeN, Bleu, Accuracy, Loss\n",
        "from ignite.utils import manual_seed, setup_logger\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.contrib.handlers import PiecewiseLinear\n",
        "from ignite.handlers import Checkpoint, global_step_from_engine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SInIPWqQmuq"
      },
      "source": [
        "# Configs\n",
        "config = {\"with_amp\": True,\n",
        "\"seed\": 126,\n",
        "\"num_epochs\": 5,\n",
        "\"batch_size\":1,\n",
        "\"output_path_\": '/content',\n",
        "\"checkpoint_every\": 100,\n",
        "\"model_name\": \"facebook/mbart-large-cc25\",\n",
        "\"tokenizer_name\": \"facebook/mbart-large-cc25\"} \n",
        "\n",
        "dataset_configs = {\"source_language\":'en', \"target_language\":'de',\"max_length\":12,\"train_dataset_length\":100000,\"validation_dataset_length\":100}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwpsSs-ef3Cc"
      },
      "source": [
        "## Preparing data\n",
        "\n",
        "We will be using the stas/wmt16-en-ro-pre-processed for this example ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjLlDIzR6QjG"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"news_commentary\", 'de-en')\n",
        "dataset = dataset.shuffle(seed=config[\"seed\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIVWhVBzJrXT"
      },
      "source": [
        "dataset = dataset[\"train\"]\n",
        "dataset = dataset.train_test_split(test_size=0.3)\n",
        "train_dataset, validation_dataset = dataset[\"train\"], dataset[\"test\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ309CblLVeh"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyVBWfpoKQXI"
      },
      "source": [
        "print(\"Example of a Datapoint\")\n",
        "print(train_dataset[0])\n",
        "print(\"Lengths\")\n",
        "print(\"\\t Train Set - {}\".format(len(train_dataset)))\n",
        "print(\"\\t Val Set - {}\".format(len(validation_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-o1azyZVQnW"
      },
      "source": [
        "tokenizer = MBartTokenizer.from_pretrained(config[\"tokenizer_name\"], src_lang=\"en_XX\", tgt_lang=\"de_DE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ5tkvn9y8Uo"
      },
      "source": [
        "class TransformerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, src_text_id, tgt_text_id, tokenizer, max_length, len):\n",
        "        self.data = data\n",
        "        self.src_text_id = src_text_id\n",
        "        self.tgt_text_id = tgt_text_id\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = [str(self.data[idx]['translation'][self.src_text_id])]\n",
        "        tgt_text = [str(self.data[idx]['translation'][self.tgt_text_id])]\n",
        "        src_text = self.tokenizer(src_text,max_length= self.max_length, padding = 'max_length',truncation=True)\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "          tgt_text = self.tokenizer(tgt_text,max_length= self.max_length,padding = 'max_length',truncation=True)\n",
        "\n",
        "        return {\n",
        "            \"src_input_ids\": torch.tensor(src_text['input_ids']).squeeze(0),\n",
        "            \"src_attention_mask\": torch.tensor(src_text['attention_mask']).squeeze(0),\n",
        "            \"tgt\": torch.tensor(tgt_text['input_ids']).squeeze(0),\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdf-SYMp1oEs"
      },
      "source": [
        "\n",
        "train_dataset = TransformerDataset(train_dataset, dataset_configs[\"source_language\"],dataset_configs[\"target_language\"],  tokenizer, dataset_configs[\"max_length\"], dataset_configs[\"train_dataset_length\"])\n",
        "val_dataset = TransformerDataset(validation_dataset, dataset_configs[\"source_language\"],dataset_configs[\"target_language\"],  tokenizer, dataset_configs[\"max_length\"], dataset_configs[\"validation_dataset_length\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKeahShANAIO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C19xqP_h9gj"
      },
      "source": [
        "## Initiating model and trainer for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTIlKn9CJGMC"
      },
      "source": [
        "# Create Trainer \n",
        "def create_trainer(model, optimizer, criterion, with_amp, train_sampler, logger):\n",
        "\n",
        "    device = idist.device()\n",
        "    scaler = GradScaler(enabled=with_amp)\n",
        "    accumulation_steps = 8\n",
        "\n",
        "    def train_step(engine, batch):\n",
        "        src_ids = batch[\"src_input_ids\"]\n",
        "        src_attention_mask = batch[\"src_attention_mask\"]\n",
        "        tgt = batch[\"tgt\"]\n",
        "\n",
        "        if src_ids.device != device:\n",
        "            src_ids = src_ids.to(device, non_blocking=True, dtype=torch.long)\n",
        "            src_attention_mask = src_attention_mask.to(device, non_blocking=True, dtype=torch.long)\n",
        "            tgt = tgt.to(device, non_blocking=True, dtype=torch.long)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with autocast(enabled=with_amp):\n",
        "            y = model(src_ids, src_attention_mask)\n",
        "            y_pred = y['logits']\n",
        "            y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "            tgt = tgt.contiguous().view(-1)\n",
        "            loss = criterion(y_pred, tgt) / accumulation_steps\n",
        "\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        if engine.state.iteration % accumulation_steps == 0:\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "        return {\n",
        "            \"batch loss\": loss.item(),\n",
        "        }\n",
        "    \n",
        "    trainer = Engine(train_step)\n",
        "    trainer.logger = logger\n",
        "\n",
        "    to_save = {\"trainer\": trainer, \"model\": model, \"optimizer\": optimizer}\n",
        "    metric_names = [\n",
        "        \"batch loss\",\n",
        "    ]\n",
        "    common.setup_common_training_handlers(\n",
        "        trainer=trainer,\n",
        "        train_sampler=train_sampler,\n",
        "        output_names=metric_names,\n",
        "        clear_cuda_cache=False,\n",
        "        with_pbars=True,\n",
        "    )\n",
        "    return trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOffHp5sOcj3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AphMNHAJoXJV"
      },
      "source": [
        "# Let's now setup evaluator engine to perform model's validation and compute metrics\n",
        "def create_evaluator(model, tokenizer, metrics, with_amp, tag=\"val\"):\n",
        "\n",
        "    device = idist.device()\n",
        "    \n",
        "    def ids_to_clean_text(generated_ids):\n",
        "        gen_text = tokenizer.batch_decode(\n",
        "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return list(map(str.strip, gen_text))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_step(engine, batch):\n",
        "        model.eval()\n",
        "\n",
        "        src_ids = batch[\"src_input_ids\"]\n",
        "        src_attention_mask = batch[\"src_attention_mask\"]\n",
        "        tgt = batch[\"tgt\"]\n",
        "\n",
        "        if src_ids.device != device:\n",
        "            src_ids = src_ids.to(device, non_blocking=True, dtype=torch.long)\n",
        "            src_attention_mask = src_attention_mask.to(device, non_blocking=True, dtype=torch.long)\n",
        "            tgt = tgt.to(device, non_blocking=True, dtype=torch.long)\n",
        "            \n",
        "        \n",
        "        y_pred = model.generate(src_ids, forced_bos_token_id=tokenizer.lang_code_to_id[\"ro_RO\"])\n",
        "        preds = ids_to_clean_text(y_pred)\n",
        "        tgt = ids_to_clean_text(tgt)\n",
        "        preds = [_preds.split() for _preds in preds]\n",
        "        tgt = [[_tgt.split()] for _tgt in tgt]\n",
        "        \n",
        "        if engine.state.iteration % 20 == 0:\n",
        "           print(\"Preds : \",preds)\n",
        "           print(\"Target : \",tgt)\n",
        "        #   print(nltk.translate.bleu_score.corpus_bleu(tgt, preds))\n",
        "\n",
        "        return preds, tgt\n",
        "\n",
        "    evaluator = Engine(evaluate_step)\n",
        "\n",
        "    for name, metric in metrics.items():\n",
        "        metric.attach(evaluator, name)\n",
        "    \n",
        "    if idist.get_rank() == 0:\n",
        "        common.ProgressBar(desc=f\"Evaluation ({tag})\", persist=False).attach(evaluator)\n",
        "\n",
        "    return evaluator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aNi7qao66YQ"
      },
      "source": [
        "def initialize():\n",
        "  model = MBartForConditionalGeneration.from_pretrained(config[\"model_name\"])\n",
        "  lr = 5e-5 * idist.get_world_size()\n",
        "  model = idist.auto_model(model)\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "  optimizer = idist.auto_optim(optimizer)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index = train_dataset.tokenizer.pad_token_id, reduction='sum')\n",
        "  # le = config[\"num_iters_per_epoch\"]\n",
        "  # milestones_values = [\n",
        "  #       (0, 0.0),\n",
        "  #       (le * warmup_epochs, lr),\n",
        "  #       (le * num_epochs, 0.0),\n",
        "  # ]\n",
        "  # lr_scheduler = PiecewiseLinear(optimizer, param_name=\"lr\", milestones_values=milestones_values)\n",
        "  return model, optimizer, criterion\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CThVRupzsxuQ"
      },
      "source": [
        "def get_dataloaders(train_dataset, val_dataset, batch_size=config[\"batch_size\"], num_workers=2):\n",
        "\n",
        "    # Setup data loader also adapted to distributed config: nccl, gloo, xla-tpu\n",
        "    train_loader = idist.auto_dataloader(\n",
        "        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = idist.auto_dataloader(\n",
        "        val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False,\n",
        "    )\n",
        "    return train_loader, val_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G1kyCtWxbZy"
      },
      "source": [
        "def log_metrics(logger, epoch, elapsed, tag, metrics):\n",
        "    metrics_output = \"\\n\".join([f\"\\t{k}: {v}\" for k, v in metrics.items()])\n",
        "    logger.info(f\"\\nEpoch {epoch} - Evaluation time (seconds): {elapsed:.2f} - {tag} metrics:\\n {metrics_output}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l0aDOM6EikW"
      },
      "source": [
        "def training(local_rank):\n",
        "\n",
        "    rank = idist.get_rank()\n",
        "    manual_seed(config[\"seed\"] + rank)\n",
        "    device = idist.device()\n",
        "\n",
        "    logger = setup_logger(name=\"NMT\", distributed_rank=local_rank)\n",
        "\n",
        "    train_loader, val_loader = get_dataloaders(train_dataset, val_dataset)\n",
        "    model, optimizer, criterion = initialize()\n",
        "\n",
        "    trainer = create_trainer(model, optimizer, criterion, config[\"with_amp\"], train_loader.sampler, logger)\n",
        "\n",
        "    metrics = {\n",
        "        \"bleu\":Bleu(ngram=4, smooth=\"smooth1\")\n",
        "      }\n",
        "\n",
        "    evaluator = create_evaluator(model, tokenizer, metrics, config[\"with_amp\"], tag=\"val\")\n",
        "    train_evaluator = create_evaluator(model, tokenizer, metrics, config[\"with_amp\"], tag=\"train\")\n",
        "    \n",
        "    @trainer.on(Events.EPOCH_COMPLETED(every=1))\n",
        "    def run_validation(engine):\n",
        "        epoch = trainer.state.epoch\n",
        "        state = evaluator.run(val_loader)\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"Validation\", state.metrics)\n",
        "\n",
        "    if rank == 0:\n",
        "      now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "      folder_name = f\"Translation_Model_backend-{idist.backend()}-{idist.get_world_size()}_{now}\"\n",
        "      output_path = Path(config[\"output_path_\"]) / folder_name\n",
        "      if not output_path.exists():\n",
        "            output_path.mkdir(parents=True)\n",
        "      \n",
        "      logger.info(f\"Output path: {output_path}\")\n",
        "    \"\"\"\n",
        "    if rank == 0:\n",
        "        # Setup TensorBoard logging on trainer and evaluators. Training Losses and Eval Metrics are logged\n",
        "        # evaluators = {\"training\": train_evaluator, \"test\": evaluator}\n",
        "        \n",
        "        # tb_logger = common.setup_tb_logging(\n",
        "        #     output_path, trainer, optimizer, evaluators=evaluators, log_every_iters=100\n",
        "        # )\n",
        "    \"\"\"\n",
        "    try:\n",
        "        state = trainer.run(train_loader, max_epochs=config[\"num_epochs\"])\n",
        "        log_metrics(logger, epoch, state.times[\"COMPLETED\"], \"Training\", state.metrics)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"\")\n",
        "        raise e\n",
        "    \n",
        "    #if rank == 0:\n",
        "    #    tb_logger.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4rGWEfQIrh5"
      },
      "source": [
        "def run():\n",
        "    with idist.Parallel(backend=None, nproc_per_node=None) as parallel:\n",
        "        parallel.run(training)\n",
        "\n",
        "run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WudDSAX-P2gK"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5hN7XDXKpLl"
      },
      "source": [
        "# lets see how our model performs\n",
        "inputs = \"अंतिम प्रविष्ट घटना को हाइलाइट करो\"\n",
        "\n",
        "translation = translator(inputs, return_text=True)\n",
        "translation = [t[\"translation_text\"] for t in translation]\n",
        "print(translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zHTeVuyfDcP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}